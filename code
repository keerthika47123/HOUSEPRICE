# house_price_prediction.py
import pandas as pd
import numpy as np
import shap
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
import matplotlib.pyplot as plt

# Simulated data loading
np.random.seed(42)
data = {
    'Location': np.random.choice(['North', 'South', 'East', 'West'], size=1000),
    'House_Type': np.random.choice(['Apartment', 'Villa', 'Townhouse'], size=1000),
    'Size': np.random.randint(800, 3000, size=1000),
    'Bedrooms': np.random.randint(1, 6, size=1000),
    'Age': np.random.randint(0, 30, size=1000),
    'CrimeRate': np.random.uniform(0.1, 10.0, size=1000),
    'SchoolRating': np.random.randint(1, 11, size=1000),
    'Price': np.random.randint(150000, 800000, size=1000)
}
df = pd.DataFrame(data)

# Feature Engineering
df['Price_per_sqft'] = df['Price'] / df['Size']
df['Distance_to_city'] = np.random.uniform(0.5, 30.0, size=1000)

# Data Cleaning
def clean_data(df):
    df = df.dropna()
    df = df.drop_duplicates()
    Q1 = df['Price'].quantile(0.25)
    Q3 = df['Price'].quantile(0.75)
    IQR = Q3 - Q1
    df = df[~((df['Price'] < (Q1 - 1.5 * IQR)) | (df['Price'] > (Q3 + 1.5 * IQR)))]
    return df

df = clean_data(df)

# Preprocessing
numerical_features = ['Size', 'Bedrooms', 'Age', 'CrimeRate', 'SchoolRating', 'Price_per_sqft', 'Distance_to_city']
categorical_features = ['Location', 'House_Type']

preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numerical_features),
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
])

# Features & target
X = df.drop('Price', axis=1)
y = df['Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Models
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.1),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(),
    "XGBoost": XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6)
}

best_score = -np.inf
best_model = None
best_model_name = ""

for name, model in models.items():
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    cv_score = np.mean(cross_val_score(pipeline, X_train, y_train, cv=5))
    
    print(f"\n{name} Performance:")
    print(f"CV Score: {cv_score:.3f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"MAE: {mae:.2f}")
    print(f"RÂ²: {r2:.3f}")
    
    if r2 > best_score:
        best_score = r2
        best_model = pipeline
        best_model_name = name

# SHAP Interpretation (only for tree-based models)
if best_model_name in ["Random Forest", "Gradient Boosting", "XGBoost"]:
    print(f"\nInterpreting model using SHAP: {best_model_name}")
    model = best_model.named_steps['model']
    preprocessed = best_model.named_steps['preprocessor'].transform(X_train)
    
    # Get feature names
    ohe_features = best_model.named_steps['preprocessor'] \
        .named_transformers_['cat'].get_feature_names_out(categorical_features)
    feature_names = numerical_features + list(ohe_features)
    
    explainer = shap.Explainer(model)
    shap_values = explainer(preprocessed)
    
    shap.summary_plot(shap_values, features=preprocessed, feature_names=feature_names, show=True)
else:
    print("\nSHAP interpretation skipped: Best model is not tree-based.")

# Save model
joblib.dump(best_model, "house_price_predictor.pkl")
print(f"\nBest model ({best_model_name}) saved successfully!")

# Flask Deployment Template
"""
from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(_name_)
model = joblib.load("house_price_predictor.pkl")

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    features = np.array([data['features']])
    prediction = model.predict(features)
    return jsonify({'prediction': prediction[0]})

if _name_ == '_main_':
    app.run(host='0.0.0.0', port=5000)
"""
